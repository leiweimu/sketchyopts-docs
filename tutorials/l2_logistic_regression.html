
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>\(\ell^2\)-regularized Logistic Regression &#8212; SketchyOpts  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=9a799264" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=8953b41a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/l2_logistic_regression';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Preconditioners" href="../api/sketchyopts.preconditioner.html" />
    <link rel="prev" title="Ridge Regression" href="ridge_regression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">SketchyOpts  documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ridge_regression.html">Ridge Regression</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#"><span class="math">\(\ell^2\)</span>-regularized Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../api/sketchyopts.preconditioner.html">Preconditioners</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/sketchyopts.solver.html">Solvers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/sketchyopts.base.html">Base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/sketchyopts.prox.html">Proximal Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/sketchyopts.util.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/sketchyopts.error.html">Error Messages</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/udellgroup/sketchyopts" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/tutorials/l2_logistic_regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>\ell^2-regularized Logistic Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-model">Fitting the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#remarks">Remarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="ell-2-regularized-logistic-regression">
<h1><span class="math">\(\ell^2\)</span>-regularized Logistic Regression<a class="headerlink" href="#ell-2-regularized-logistic-regression" title="Link to this heading">#</a></h1>
<p>In this tutorial section, we demonstrate an <span class="math">\(\ell^2\)</span>-regularized logistic regression example using SketchyOpts. We first introduce some useful notations.</p>
<!-- - $X \in \mathbb{R}^{n \times p}$: input features (or design matrix)
- $y \in \{-1, +1\}^{n}$: target classes
- $(x_i, y_i) \in \mathbb{R}^p \times \{-1, +1\}$: $i$<sup>th</sup> training sample
- $\beta = [\beta_0, \beta_{1}^{\mathsf{T}}] \in \mathbb{R}^{p+1}$: parameters of the model (including both the weights $\beta_1$ and bias $\beta_0$)
- $\lambda \geqslant 0$: regularization strength of the model -->
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math">\(X \in \mathbb{R}^{n \times p}\)</span></p></td>
<td><p>input features (or design matrix)</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math">\(y \in \{-1, +1\}^{n}\)</span></p></td>
<td><p>target classes</p></td>
</tr>
<tr class="row-even"><td><p><span class="math">\((x_i, y_i) \in \mathbb{R}^p \times \{-1, +1\}\)</span></p></td>
<td><p><span class="math">\(i\)</span><sup>th</sup> training sample</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math">\(\beta = [\beta_0, \beta_{1}^{\mathsf{T}}] \in \mathbb{R}^{p+1}\)</span></p></td>
<td><p>parameters of the model (including both the weights <span class="math">\(\beta_1\)</span> and bias <span class="math">\(\beta_0\)</span>)</p></td>
</tr>
<tr class="row-even"><td><p><span class="math">\(\lambda \geqslant 0\)</span></p></td>
<td><p>regularization strength of the model</p></td>
</tr>
</tbody>
</table>
</div>
<p>Logistic regression is a discriminative classification model that seeks to represent posterior probabilities of <span class="math">\(K\)</span> classes using linear functions of the input features. In this example, we focus on the binary case in which <span class="math">\(K = 2\)</span>. The model has the form
<div class="math">
\[
    \log \frac{p(y = +1 \mid x)}{p(y = -1 \mid x)} = \beta_0 + \beta_1^{\mathsf{T}} x
\]</div>
</p>
<p>The above log-odd and the constrain that the probabilities sum to one together suggest the probability of each label is computed as
<div class="math">
\[
\begin{aligned}
    & p(y = -1 \mid x) = \frac{1}{1 + \exp(\beta_0 + \beta_1^{\mathsf{T}} x)} \\
    & p(y = +1 \mid x) = \frac{\exp(\beta_0 + \beta_1^{\mathsf{T}} x)}{1 + \exp(\beta_0 + \beta_1^{\mathsf{T}} x)}
\end{aligned}
\]</div>

which can also be compactly written using sigmoid function <span class="math">\(\sigma(x) = e^{x} / (1 + e^{x})\)</span> as <span class="math">\(\sigma\big(y \cdot (\beta_0 + \beta_1^{\mathsf{T}} x)\big)\)</span>.</p>
<p>To estimate the parameters of the logistic regression model, we usually use maximum likelihood estimation. Using negative log-likelihood, we pose the <span class="math">\(\ell^2\)</span>-regularized logistic regression as the following optimization problem
<div class="math">
\[
    \underset{\beta}{\operatorname{minimize}} ~ \frac{1}{n} \sum_{i=1}^{n} \log \Big(1 + \exp\big(-y_i (\beta_0 + \beta_1^{\mathsf{T}} x_i)\big)\Big) + \frac{\lambda}{2} \lVert \beta_1 \rVert_2^2
\]</div>

The additional regularization term introduces bias to the model with the hope to decrease the variance and temper overfitting (note that only the weights <span class="math">\(\beta_1\)</span> get penalized). The regularization strength <span class="math">\(\lambda\)</span> is often chosen via cross-validation to ensure the fitted model generalizes well to the unseen (test) data.</p>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">#</a></h2>
<p>In this example, we use the <code class="docutils literal notranslate"><span class="pre">real-sim</span></code> dataset from the <a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html">LIBSVM data collection</a> <span id="id1">[<a class="reference internal" href="#id10" title="Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):1–27, may 2011. URL: https://doi.org/10.1145/1961189.1961199, doi:10.1145/1961189.1961199.">Chang and Lin, 2011</a>]</span>. The dataset was originally gathered by <a class="reference external" href="https://people.cs.umass.edu/~mccallum/data.html">Andrew McCallum</a> as a part of a larger collection of UseNet discussion group articles. The dataset has 72,309 samples and each sample has 20,958 features. We obtain the dataset from <a class="reference external" href="https://www.openml.org/d/1578">OpenML</a> and use a random 80/20 split to construct training and test subsets.</p>
<p>Given that the dataset is a document set, a common pre-processing practice in text classification is normalizing each sample to unit length. Fortunately, the dataset comes pre-processed. Moreover, the labels are already <span class="math">\(\pm 1\)</span>-encoded. We thus simply add a constant 1 column to the data matrix to incoporate the bias.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The data matrix in this example is sparse. Instead of densifying it, we take advantage of the experimental <a class="reference external" href="https://jax.readthedocs.io/en/latest/jax.experimental.sparse.html">JAX sparse module</a>. To enable sparsification of the PROMISE solver, one needs to set argument <code class="code docutils literal highlight highlight-python"><span class="n">sparse</span> <span class="o">=</span> <span class="kc">True</span></code> when initializing the solver. Moreover, any function that gets passed into the solver (<em>e.g.</em> objective function <code class="docutils literal notranslate"><span class="pre">fun</span></code> etc.) needs also be compatible with sparse data input.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Since the JAX sparse module is currently under development, the sparsification feature of SketchyOpts is preliminary and might not be performant. We plan to gradually improve its performance once the support for sparse matrix operations in JAX is finalized and becomes official.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># set seed for reproducibility</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># download dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_openml</span><span class="p">(</span><span class="n">data_id</span><span class="o">=</span><span class="mi">1578</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># split into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">n_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">n_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># include bias</span>

<span class="c1"># incorporate bias into the data (prefix a constant 1 column)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_train</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">X_train</span><span class="p">])</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_test</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">X_test</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="fitting-the-model">
<h2>Fitting the model<a class="headerlink" href="#fitting-the-model" title="Link to this heading">#</a></h2>
<p>We first define the objective function of the optimization problem introduced earlier. We also implement a callback function that gets called to help us keep track of the loss and misclassification rates before each update of the parameters in the optimization loop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">sparse</span>

<span class="c1"># define objective evaluation function</span>
<span class="nd">@sparse</span><span class="o">.</span><span class="n">sparsify</span>
<span class="k">def</span> <span class="nf">compute_objective</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sparse</span><span class="o">.</span><span class="n">todense</span><span class="p">(</span><span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">params</span><span class="p">)))))</span> <span class="o">+</span> <span class="p">(</span><span class="n">reg</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

<span class="nd">@sparse</span><span class="o">.</span><span class="n">sparsify</span>
<span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span> 
    <span class="c1"># slice data array to get features and classes</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># compute objective</span>
    <span class="n">obj</span> <span class="o">=</span> <span class="n">compute_objective</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">obj</span>

<span class="c1"># define function that computes the error rate</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">compute_error_rate</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span> 
    <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">params</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">!=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">scores</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># initialize empty containers</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_error</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># define the callback function</span>
<span class="n">eval_freq</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">def</span> <span class="nf">callback</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span> 
    <span class="c1"># perform computations every 5 iterations</span>
    <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">%</span> <span class="n">eval_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> 
        <span class="c1"># record training loss</span>
        <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">compute_objective</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">sparse</span><span class="o">.</span><span class="n">BCOO</span><span class="o">.</span><span class="n">from_scipy_sparse</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">reg</span><span class="p">))</span>

        <span class="c1"># evaluate both training and test errors</span>
        <span class="n">train_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compute_error_rate</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">sparse</span><span class="o">.</span><span class="n">BCOO</span><span class="o">.</span><span class="n">from_scipy_sparse</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">))</span>
        <span class="n">test_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compute_error_rate</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">sparse</span><span class="o">.</span><span class="n">BCOO</span><span class="o">.</span><span class="n">from_scipy_sparse</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">state</span>
</pre></div>
</div>
</div>
</div>
<p>We use SketchyKatyusha (with the Nyström subsampled Newton preconditioner) to fit the model. Rank and regularization for the preconditioner are set to their default values (<span class="math">\(10\)</span> and <span class="math">\(10^{-3}\)</span> respectively). Since the objective has a non-constant Hessian, we set the preconditioner update frequency to the recommended <span class="math">\(\lceil n/b_g \rceil\)</span> (where <span class="math">\(b_g\)</span> is the stochastic gradient batch size) so that the preconditioner is computed after each equivalent full pass of the training set. The rest of the hyperparameters also follow the recommended values discussed in the PROMISE paper <span id="id2">[<a class="reference internal" href="#id8" title="Zachary Frangella, Pratik Rathore, Shipu Zhao, and Madeleine Udell. PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates. 2024. URL: https://arxiv.org/abs/2309.02014, arXiv:2309.02014.">Frangella <em>et al.</em>, 2024</a>]</span>. In this example, we arbitrarily set the regularization strength <span class="math">\(\lambda = 0.01/n\)</span> to demonstrate the performance of SketchyKatyusha as this small value is not likely to meaningfully improve the conditioning of the problem which makes it more challenging for the optimizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sketchyopts.solver</span> <span class="kn">import</span> <span class="n">SketchyKatyusha</span>

<span class="c1"># specify hyperparameters</span>
<span class="n">grad_batch_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">hess_batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_train</span><span class="p">)))</span>
<span class="n">update_freq</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">n_train</span> <span class="o">/</span> <span class="n">grad_batch_size</span><span class="p">))</span>
<span class="n">snapshop_update_prob</span> <span class="o">=</span> <span class="n">grad_batch_size</span> <span class="o">/</span> <span class="n">n_train</span>
<span class="n">maxiter</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-6</span>

<span class="n">init_params</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">reg</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">/</span> <span class="n">n_train</span>

<span class="c1"># initialize the solver and start the optimization</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">SketchyKatyusha</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">fun</span><span class="p">,</span> 
                      <span class="n">pre_update</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> 
                      <span class="n">grad_batch_size</span><span class="o">=</span><span class="n">grad_batch_size</span><span class="p">,</span> 
                      <span class="n">hess_batch_size</span><span class="o">=</span><span class="n">hess_batch_size</span><span class="p">,</span> 
                      <span class="n">update_freq</span><span class="o">=</span><span class="n">update_freq</span><span class="p">,</span> 
                      <span class="n">snapshop_update_prob</span><span class="o">=</span><span class="n">snapshop_update_prob</span><span class="p">,</span>
                      <span class="n">mu</span><span class="o">=</span><span class="n">reg</span><span class="p">,</span>
                      <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> 
                      <span class="n">maxiter</span><span class="o">=</span><span class="n">maxiter</span><span class="p">,</span> 
                      <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> 
                      <span class="n">jit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                      <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">params</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_params</span><span class="p">,</span> <span class="n">sparse</span><span class="o">.</span><span class="n">BCOO</span><span class="o">.</span><span class="n">from_scipy_sparse</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">reg</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We plot suboptimality gaps and misclassification rates from the run. The optimal solution is obtained by running L-BFGS <span id="id3">[<a class="reference internal" href="#id11" title="Dong C. Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45:503-528, 1989. URL: https://doi.org/10.1007/BF01589116.">Liu and Nocedal, 1989</a>]</span> with <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">scikit-learn API</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># obtain the optimal solution</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">reg</span> <span class="o">*</span> <span class="n">n_train</span><span class="p">),</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">250</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()[:,</span><span class="mi">1</span><span class="p">:],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">opt_val</span> <span class="o">=</span> <span class="n">compute_objective</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]),</span> <span class="n">sparse</span><span class="o">.</span><span class="n">BCOO</span><span class="o">.</span><span class="n">from_scipy_sparse</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>

<span class="c1"># update plot settings</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_formats = [&#39;svg&#39;]
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">})</span>

<span class="c1"># manually evaluate the resulting parameters</span>
<span class="c1"># as the callback only gets called before each update</span>
<span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compute_objective</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">sparse</span><span class="o">.</span><span class="n">BCOO</span><span class="o">.</span><span class="n">from_scipy_sparse</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">reg</span><span class="p">))</span>
<span class="n">train_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compute_error_rate</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">sparse</span><span class="o">.</span><span class="n">BCOO</span><span class="o">.</span><span class="n">from_scipy_sparse</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">))</span>
<span class="n">test_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compute_error_rate</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">sparse</span><span class="o">.</span><span class="n">BCOO</span><span class="o">.</span><span class="n">from_scipy_sparse</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># compute suboptimality gap</span>
<span class="n">subopt_gap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span> <span class="o">-</span> <span class="n">opt_val</span>

<span class="c1"># make subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">subplot_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">box_aspect</span><span class="o">=</span><span class="mf">0.45</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">h_pad</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># plot suboptimality gap]</span>
<span class="n">iter_nums</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">eval_freq</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">eval_freq</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">iter_nums</span><span class="p">,</span> <span class="n">subopt_gap</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Suboptimality gap&#39;</span><span class="p">)</span>

<span class="c1"># plot training and test errors</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iter_nums</span><span class="p">,</span> <span class="n">train_error</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training error&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iter_nums</span><span class="p">,</span> <span class="n">test_error</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test error&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Misclassification rate&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e48d5457ef20dd27c6e63911f2d2e2896a7abbbb84675ed04ac45f1f44fc1f82.svg" class="align-center" src="../_images/e48d5457ef20dd27c6e63911f2d2e2896a7abbbb84675ed04ac45f1f44fc1f82.svg" />
</div>
</div>
<p>The objective suboptimality for SketchyKatyusha appears to have decreased at a linear rate. We note that running the solver for 500 iterations is equivalent to going through the entire training set for less than 5 epochs; based on the experiment results from the PROMISE paper <span id="id4">[<a class="reference internal" href="#id8" title="Zachary Frangella, Pratik Rathore, Shipu Zhao, and Madeleine Udell. PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates. 2024. URL: https://arxiv.org/abs/2309.02014, arXiv:2309.02014.">Frangella <em>et al.</em>, 2024</a>]</span>, with extra optimization time budget, the gap would eventually reach machine precision.</p>
</section>
<section id="remarks">
<h2>Remarks<a class="headerlink" href="#remarks" title="Link to this heading">#</a></h2>
<p>Lastly, we would like to demonstrate the performance gain from enabling JIT-compilation. For demonstration purpose, we bring down the number of training samples to 640 and run the solver for 10 iterations on CPU. Although the problem scale is dramatically reduced from our running example, we expect the pattern to persist, if not magnify with more training samples, extended number of iterations, or on accelerated computing instances. For this reason, we recommend running PROMISE solvers with <code class="code docutils literal highlight highlight-python"><span class="n">jit</span> <span class="o">=</span> <span class="kc">True</span></code> (which is the default setting) whenever possible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># specify demo parameters</span>
<span class="n">demo_num_samples</span> <span class="o">=</span> <span class="mi">640</span>
<span class="n">demo_grad_batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">demo_hess_batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">demo_update_freq</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">demo_snapshop_update_prob</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">demo_maxiter</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">demo_reg</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">/</span> <span class="n">demo_num_samples</span>

<span class="c1"># subsample the original training data</span>
<span class="n">demo_data</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">BCOO</span><span class="o">.</span><span class="n">from_scipy_sparse</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()[:</span><span class="n">demo_num_samples</span><span class="p">,:])</span>

<span class="c1"># initialize JIT-enabled solver</span>
<span class="n">opt_jitted</span> <span class="o">=</span> <span class="n">SketchyKatyusha</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">fun</span><span class="p">,</span> 
                      <span class="n">grad_batch_size</span><span class="o">=</span><span class="n">demo_grad_batch_size</span><span class="p">,</span> 
                      <span class="n">hess_batch_size</span><span class="o">=</span><span class="n">demo_hess_batch_size</span><span class="p">,</span> 
                      <span class="n">update_freq</span><span class="o">=</span><span class="n">demo_update_freq</span><span class="p">,</span> 
                      <span class="n">snapshop_update_prob</span><span class="o">=</span><span class="n">demo_snapshop_update_prob</span><span class="p">,</span>
                      <span class="n">mu</span><span class="o">=</span><span class="n">demo_reg</span><span class="p">,</span>
                      <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> 
                      <span class="n">maxiter</span><span class="o">=</span><span class="n">demo_maxiter</span><span class="p">,</span> 
                      <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> 
                      <span class="n">jit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                      <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># initialize JIT-disabled solver</span>
<span class="n">opt_non_jitted</span> <span class="o">=</span> <span class="n">SketchyKatyusha</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">fun</span><span class="p">,</span> 
                      <span class="n">grad_batch_size</span><span class="o">=</span><span class="n">demo_grad_batch_size</span><span class="p">,</span> 
                      <span class="n">hess_batch_size</span><span class="o">=</span><span class="n">demo_hess_batch_size</span><span class="p">,</span> 
                      <span class="n">update_freq</span><span class="o">=</span><span class="n">demo_update_freq</span><span class="p">,</span> 
                      <span class="n">snapshop_update_prob</span><span class="o">=</span><span class="n">demo_snapshop_update_prob</span><span class="p">,</span>
                      <span class="n">mu</span><span class="o">=</span><span class="n">demo_reg</span><span class="p">,</span>
                      <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> 
                      <span class="n">maxiter</span><span class="o">=</span><span class="n">demo_maxiter</span><span class="p">,</span> 
                      <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                      <span class="n">jit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;JIT-enabled solver:&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> _ = opt_jitted.run(init_params, demo_data, demo_reg)

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;JIT-disabled solver:&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> _ = opt_non_jitted.run(init_params, demo_data, demo_reg)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>JIT-enabled solver:
14.8 s ± 175 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
JIT-disabled solver:
26.8 s ± 178 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</pre></div>
</div>
</div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id5">
<div role="list" class="citation-list">
<div class="citation" id="id10" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">LR1</a><span class="fn-bracket">]</span></span>
<p>Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. <em>ACM Transactions on Intelligent Systems and Technology</em>, 2(3):1–27, may 2011. URL: <a class="reference external" href="https://doi.org/10.1145/1961189.1961199">https://doi.org/10.1145/1961189.1961199</a>, <a class="reference external" href="https://doi.org/10.1145/1961189.1961199">doi:10.1145/1961189.1961199</a>.</p>
</div>
<div class="citation" id="id8" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LR2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id4">2</a>)</span>
<p>Zachary Frangella, Pratik Rathore, Shipu Zhao, and Madeleine Udell. PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2309.02014">https://arxiv.org/abs/2309.02014</a>, <a class="reference external" href="https://arxiv.org/abs/2309.02014">arXiv:2309.02014</a>.</p>
</div>
<div class="citation" id="id11" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">LR3</a><span class="fn-bracket">]</span></span>
<p>Dong C. Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. <em>Mathematical Programming</em>, 45:503–528, 1989. URL: <a class="reference external" href="https://doi.org/10.1007/BF01589116">https://doi.org/10.1007/BF01589116</a>.</p>
</div>
</div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ridge_regression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Ridge Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="../api/sketchyopts.preconditioner.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Preconditioners</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-model">Fitting the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#remarks">Remarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Author name not set
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>