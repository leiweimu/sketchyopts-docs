{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3dc80d6-6d90-4242-bbfb-bce5aec9f52b",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "In this tutorial section, we use SketchyOpts to train a support vector machine (SVM) classifier. We first introduce some useful notations. \n",
    "| Symbol | Description |\n",
    "| --- | --- |\n",
    "| $X \\in \\mathbb{R}^{n \\times p}$ | input features (or design matrix) |\n",
    "| $y \\in \\{-1, +1\\}^{n}$ | target classes |\n",
    "| $(x_i, y_i) \\in \\mathbb{R}^p \\times \\{-1, +1\\}$ | $i$<sup>th</sup> training sample |\n",
    "| $\\beta = [\\beta_0, \\beta_{1}^{\\mathsf{T}}] \\in \\mathbb{R}^{p+1}$ | parameters of the model (including both the weights $\\beta_1$ and bias $\\beta_0$) |\n",
    "| $C \\geqslant 0$ | penalty parameter for misclassification |\n",
    "\n",
    "SVM classifies data by maximizing the \"margin\" around the hyperplane in feature space that separates all data points of one class from those of the other class. The margin, more precisely, is the minimum distance between sample instances and the decision boundary. Often times the dataset we encounter is nonseparable. To address such difficulty and still be able to make classification decisions, we instead allow sample instances to be on the wrong side of the margin (also called soft-margin SVM). This gives rise to the following optimization problem\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\underset{\\beta, \\, \\{\\xi_i\\}_{i=1}^{n}}{\\operatorname{minimize}} \\quad& \\frac{1}{2} \\lVert \\beta_1 \\rVert_2^2 + C \\, \\sum_{i=1}^{n} \\xi_i \\\\\n",
    "    \\text{subject to} \\quad& y_i(x_i^{\\mathsf{T}}\\beta_1 + \\beta_0) \\geqslant 1 - \\xi_i, \\, \\forall i \\\\\n",
    "    & \\xi_i \\geqslant 0, \\, \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here $(\\xi_1, \\cdots, \\xi_n)$ are slack variables characterizing the amount by which the prediction $x_i^{\\mathsf{T}}\\beta_1 + \\beta_0$ is on the wrong side of its margin. \n",
    "\n",
    "- When $\\xi_i = 0$, the prediction is outside the margin on the correct side of the separating hyperplane;\n",
    "- when $0 < \\xi_i \\leqslant 1$, the prediction falls within the margin on the correct side of the separating hyperplane;\n",
    "- when $\\xi_i > 1$, the prediction lies on the wrong side of the separating hyperplane. \n",
    "\n",
    "The objective seeks to balance the goals of finding a large margin and ensuring low training classification error. The parameter $C$ controls the relative weighting between the two. In practice, the value for $C$ is often determined by cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981c3f9-97b4-44b7-9a3b-e2d4d4d930fe",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We use the `p53 Mutants` {cite}`svm-lathrop2009p53` dataset from [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/188/p53+mutants). The dataset has 16,772 samples and each sample has 5,408 features assembled from biophysical simulations, and a binary label (transcriptonally competent, active p53 vs. cancerous, inactive p53) determined via in vivo assays. The goal is to use these 2D electrostatic and surface based as well as 3D distance based features to predict mutant p53 transcriptional activity. \n",
    "\n",
    "We download the dataset, and form training and test subsets with random 70-30 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22c0c80-baad-4a1e-afb4-0c91ad08bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import zipfile, os\n",
    "import pandas as pd\n",
    "\n",
    "# set seed for reproducibility\n",
    "seed = 0\n",
    "\n",
    "# download the dataset\n",
    "url = 'https://archive.ics.uci.edu/static/public/188/p53+mutants.zip'\n",
    "file_Path = 'p53+mutants.zip'\n",
    "urlretrieve(url, file_Path)\n",
    "\n",
    "# extract datafile\n",
    "with zipfile.ZipFile(file_Path, 'r') as zip_ref:\n",
    "    zip_ref.extract('p53_old_2010.zip', '')\n",
    "with zipfile.ZipFile('p53_old_2010.zip', 'r') as zip_ref:\n",
    "    with zip_ref.open('p53_old_2010/K8.data') as f:\n",
    "        # need to ignore last column because the datafile \n",
    "        # contains an extra comma at the end of each line\n",
    "        df = pd.read_csv(f, usecols=range(5409), header=None, na_values='?')\n",
    "os.remove('p53_old_2010.zip') # clean up extracted file\n",
    "\n",
    "# remove samples with missing value\n",
    "df = df.dropna()\n",
    "\n",
    "# convert string labels to binary {-1, +1}\n",
    "df[df.columns[-1]] = df.iloc[:,-1].map({'inactive': -1.0, 'active': 1.0})\n",
    "\n",
    "# perform random split\n",
    "train_df = df.sample(frac=0.7, random_state=seed)\n",
    "test_df = df.drop(train_df.index)\n",
    "X_train = train_df.iloc[:,:-1].to_numpy()\n",
    "y_train = train_df.iloc[:,-1].to_numpy()\n",
    "X_test = test_df.iloc[:,:-1].to_numpy()\n",
    "y_test = test_df.iloc[:,-1].to_numpy()\n",
    "del df, train_df, test_df\n",
    "\n",
    "# get the shape of training samples\n",
    "n, p = X_train.shape\n",
    "n_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c36f17-314b-4efb-a9e6-2d990b915c7b",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "To train the SVM classifier, we work with the dual problem\n",
    "$$\n",
    "\\begin{aligned}\n",
    "     \\underset{\\alpha}{\\operatorname{maximize}} \\quad& \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i,j}^{n} \\alpha_i \\alpha_j y_i y_j x_i^{\\mathsf{T}} x_j \\\\\n",
    "     \\text{subject to} \\quad& \\sum_{i=1}^{n} \\alpha_i y_i = 0 \\\\\n",
    "     & 0 \\leqslant \\alpha_i \\leqslant C, \\, \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We see the objective involves the input features only via inner products. This enables us to use a more flexible variant of SVM that enlarges the feature space using basis expansion with kernel functions. The resulting classifier is capable of finding non-linear boundaries. The updated optimization problem is the following\n",
    "$$\n",
    "\\begin{aligned}\n",
    "     \\underset{\\alpha}{\\operatorname{maximize}} \\quad& \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i,j}^{n} \\alpha_i \\alpha_j y_i y_j \\langle \\phi(x_i), \\, \\phi(x_j) \\rangle \\\\\n",
    "     \\text{subject to} \\quad& \\sum_{i=1}^{n} \\alpha_i y_i = 0 \\\\\n",
    "     & 0 \\leqslant \\alpha_i \\leqslant C, \\, \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\phi(\\cdot)$ is the kernel function that transforms feature vectors. The above problem can be expressed more succinctly (as in the form presented by ), \n",
    "$$\n",
    "\\begin{aligned}\n",
    "     \\underset{\\alpha}{\\operatorname{minimize}} \\quad& \\frac{1}{2} \\alpha^{\\mathsf{T}} \\operatorname{diag}(y) K \\operatorname{diag}(y) \\alpha - \\vec{1}^{\\mathsf{T}}\\alpha \\\\\n",
    "     \\text{subject to} \\quad& y^{\\mathsf{T}}\\alpha = 0 \\\\\n",
    "     & 0 \\leqslant \\alpha_i \\leqslant C, \\, \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $K_{i,j} = K(x_i, x_j) = \\langle \\phi(x_i), \\, \\phi(x_j) \\rangle$. We use the radial basis function (RBF) kernel in this example, and thus $K(\\cdot, \\cdot)$ takes the form\n",
    "$$\n",
    "    K(x,x') = \\exp(-\\gamma \\lVert x - x' \\rVert_2^2)\n",
    "$$\n",
    "where $\\gamma > 0$ is a parameter that sets the kernel width. A smaller value of $\\gamma$ leads to more smooth decision boundaries. \n",
    "\n",
    "Here we use the [scikit-learn RBF kernel](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.pairwise.rbf_kernel.html) implementation and, precompute the Gram matrix $G \\coloneqq \\operatorname{diag}(y) K \\operatorname{diag}(y)$ with kernel width set to $\\gamma = 1/p$ (default value of the implementation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705a3699-93d0-423e-9003-717b6caafb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "# precompute kernel and Gram matrices\n",
    "K = rbf_kernel(X_train)\n",
    "G = jnp.diag(y_train) @ K @ jnp.diag(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959c4ae-bf08-475d-b617-dce3152a20d4",
   "metadata": {},
   "source": [
    "We demonstrate how to set up NysADMM to train the SVM classifier. To apply NysADMM, we need to reformulate the problem and split the objective into separate components. One way of doing this is to define\n",
    "\n",
    "- ``fun``: $\\frac{1}{2} \\alpha^{\\mathsf{T}} \\operatorname{diag}(y) K \\operatorname{diag}(y) \\alpha$\n",
    "- ``reg_g``: $-\\vec{1}^{\\mathsf{T}}\\alpha$\n",
    "- ``reg_h``: $1_{\\mathcal{C}}$ where $\\mathcal{C} = \\{\\alpha \\in \\mathbb{R}^{n} \\mid y^{\\mathsf{T}}\\alpha = 0, \\, 0 \\leqslant \\alpha_i \\leqslant C, \\, \\forall i\\}$ is the intersection of a hyperplane and a hypercube\n",
    "\n",
    "The proximal operator of the indicator function $1_{\\mathcal{C}}$ is the projection onto $\\mathcal{C}$, which is given by\n",
    "$$\n",
    "    \\operatorname{prox}_{\\lambda \\cdot 1_{\\mathcal{C}}}(z) = \\Pi_{[0,C]}(z - \\mu^{\\ast}y)\n",
    "$$\n",
    "where $\\Pi_{[0,C]}(\\cdot)$ is the projection onto the hypercube\n",
    "$$\n",
    "    \\big[\\Pi_{[0,C]}(z)\\big]_{j} = \n",
    "    \\begin{cases}\n",
    "        ~ 0 & \\text{if}~ z_j < 0 \\\\\n",
    "        ~ z_j & \\text{if}~0 \\leqslant z_j \\leqslant C \\\\\n",
    "        ~ C & \\text{if}~ z_j > C \\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "and $\\mu^{\\ast}$ is the root of \n",
    "$$\n",
    "    \\varphi(\\mu) \\coloneqq y^{\\mathsf{T}}\\Pi_{[0,C]}(z - \\mu y)\n",
    "$$\n",
    "We can use bisection method to find $\\mu^{\\ast}$. We implement these component functions along with related oracles and the aforementioned proximal operator.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc563d29-ea4b-4934-b90a-7f775d62761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "# define the objective `fun`\n",
    "def fun(params, data): \n",
    "    return 0.5 * jnp.dot(params, data @ params)\n",
    "\n",
    "# define the gradient oracle\n",
    "def grad_fun(params, data): \n",
    "    return data @ params\n",
    "\n",
    "# define the hvp oracle\n",
    "def hvp_fun(params, vec, data): \n",
    "    return data @ vec\n",
    "\n",
    "# define the smooth `reg_g`\n",
    "def reg_g(params): \n",
    "    return -jnp.sum(params)\n",
    "\n",
    "# define the proximal operator of the non-smooth `reg_h`\n",
    "def prox_reg_h(point, scaling, C): \n",
    "    del scaling\n",
    "\n",
    "    # define hypercube parameters\n",
    "    lower = 0.0\n",
    "    upper = C\n",
    "\n",
    "    # define hyperplane parameters\n",
    "    coeffs = y_train\n",
    "    scalar = 0.0\n",
    "\n",
    "    # define projection onto hypercube\n",
    "    def hypercube_proj(x): \n",
    "        return jnp.minimum(jnp.maximum(x, lower), upper)\n",
    "\n",
    "    # define bisection objective \n",
    "    def phi(mu):\n",
    "        return jnp.dot(coeffs, hypercube_proj(point - mu * coeffs)) - scalar\n",
    "\n",
    "    # identify bisection bracket\n",
    "    bisect_lower = jax.lax.while_loop(lambda value: phi(value) < 0, lambda value: value * 2, -1)\n",
    "    bisect_upper = jax.lax.while_loop(lambda value: phi(value) > 0, lambda value: value * 2, 1)\n",
    "\n",
    "    # find optimal mu\n",
    "    max_iter = 100\n",
    "    atol = 1e-5\n",
    "    rtol = 4 * jnp.finfo('float32').eps\n",
    "    \n",
    "    def cond_fun(values): \n",
    "        iter_num, l, u = values\n",
    "        c = (l + u) / 2\n",
    "        return (iter_num < max_iter) & (phi(c) != 0) & (jnp.abs((l - u) / 2) >= atol + rtol * jnp.abs(c))\n",
    "\n",
    "    def body_fun(values): \n",
    "        iter_num, l, u = values\n",
    "        c = (l + u) / 2\n",
    "        r = (jnp.sign(phi(c)) * jnp.sign(phi(l)) + 1) / 2\n",
    "        return iter_num + 1, (1 - r) * l + r * c, r * u + (1 - r) * c\n",
    "\n",
    "    _, l, u = jax.lax.while_loop(cond_fun, body_fun, (0, bisect_lower, bisect_upper))\n",
    "    mu = (l + u) / 2\n",
    "\n",
    "    # compute projection\n",
    "    y = hypercube_proj(point - mu * coeffs)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfeecf3-07aa-46ea-a474-30f85ff8a8cd",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Besides the built-in SketchyOpts proximal operations there are dedicated libraries for computing proximal mappings. For instance, [PyProx](https://pyproximal.readthedocs.io/en/stable/api/generated/pyproximal.projection.HyperPlaneBoxProj.html) implements the relevant projection that can be adapted for our use. \n",
    "\n",
    "However, we might need to make sure these external implementations are compatible with JAX. Here we choose to implement the proximal operator ourselves because `HyperPlaneBoxProj` from PyProx does not work well with JIT-compilation. \n",
    "```\n",
    "\n",
    "We fit the model using NysADMM with sketch size $20$. As discussed in the NysADMM paper {cite}`svm-zhao2022nysadmm`, the Gram matrix $\\operatorname{diag}(y) K \\operatorname{diag}(y)$ is approximately low rank, and we would expect NysADMM to enjoy accelerated convergence in practice even if the effective dimension of the Hessian of the ADMM subproblem is greater than the sketch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087a5a31-e94f-4a1c-99be-38452b174957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sketchyopts.solver import NysADMM\n",
    "\n",
    "# define optimizer parameters\n",
    "sketch_size = 20\n",
    "step_size = 500\n",
    "maxiter = 100\n",
    "init_params = jnp.zeros(n)\n",
    "C = 1.0 / n\n",
    "\n",
    "# run NysADMM to fit SVM classifier\n",
    "opt = NysADMM(fun=fun, \n",
    "              grad_fun=grad_fun, \n",
    "              hvp_fun=hvp_fun, \n",
    "              reg_g=reg_g, \n",
    "              prox_reg_h=prox_reg_h, \n",
    "              step_size=step_size, \n",
    "              sketch_size=sketch_size, \n",
    "              maxiter=maxiter)\n",
    "params, state = opt.run(init_params, G, prox_reg_h_params={'C': C})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37d254-b681-4090-befc-dcd71d3078dc",
   "metadata": {},
   "source": [
    "To evaluate the test accuracy of the fitted classifier, we first recover optimal primal variables $\\beta^{\\ast}$ from the obtained optimal dual variables $\\alpha^{\\ast}$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\beta_1^{\\ast} = \\sum_{i=1}^{n} \\alpha_i^{\\ast} y_i \\phi(x_i) \\\\\n",
    "    & \\beta_0^{\\ast} = y_k - (\\beta_1^{\\ast})^{\\mathsf{T}} \\phi(x_k) \\quad \\text{for any}~k~\\text{such that}~0 < \\alpha_k^{\\ast} < C\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For numerical stability, we take the average of all such $(x_k, y_k, \\alpha_k^{\\ast})$'s when computing $\\beta_0^{\\ast}$. \n",
    "\n",
    "Then we can make predictions on the unseen example $x$ using recovered $\\beta^{\\ast}$ by the following decision rule\n",
    "$$\n",
    "    \\operatorname{sign}\\big((\\beta_1^{\\ast})^{\\mathsf{T}} \\phi(x) + \\beta_0^{\\ast}\\big)\n",
    "    = \\operatorname{sign}\\bigg(\\sum_{i=1}^{n} \\alpha_i^{\\ast} y_i K(x, x_i) + \\beta_0^{\\ast}\\bigg)\n",
    "$$\n",
    "where $K(x, x_i) = \\langle \\phi(x), \\phi(x_i) \\rangle$. We now implement prediction function and compute the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecdc4c7-431d-4e94-a84a-bf532db84e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 99.28%\n"
     ]
    }
   ],
   "source": [
    "# compute new kernel matrix for testing\n",
    "K_test = rbf_kernel(X_test, X_train)\n",
    "\n",
    "# define prediction function\n",
    "def predict(params, kernel): \n",
    "    # recover bias\n",
    "    idx = jnp.argwhere(params[(0 < params) & (params < C)]).flatten()\n",
    "    bias = jnp.mean(y_train[idx] - K[idx,:] @ (params * y_train))\n",
    "    # make predictions\n",
    "    preds = jnp.sign(kernel @ (params * y_train) + bias)\n",
    "    return preds\n",
    "    \n",
    "# define test error evaluation function\n",
    "def eval_test_error(preds): \n",
    "    return jnp.sum(0.5 * jnp.abs(y_test - preds)) / n_test\n",
    "\n",
    "# output result\n",
    "test_accuracy = 1.0 - eval_test_error(predict(params, K_test))\n",
    "print('Test accuracy: {:.2f}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc7159-2560-4fbf-8e41-eeab56783da7",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "In this example, we see NysADMM is able to converge fast. Here we compare the training time to the [scikit-learn SVM](https://scikit-learn.org/dev/modules/generated/sklearn.svm.SVC.html) implementation which is based on LIBSVM {cite}`svm-chang2011libsvm`. As illustrated in the paper {cite}`svm-zhao2022nysadmm`, NysADMM is expected to have great performance when the Hessian of the ADMM subproblem is dense and approximately low rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1a185f-f9d3-49df-9210-b0c62f399ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NysADMM:\n",
      "13.3 s ± 103 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "SVC:\n",
      "1min 6s ± 422 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# include precompute time for fair comparison\n",
    "def solve_with_NysADMM(): \n",
    "    G = jnp.diag(y_train) @ rbf_kernel(X_train) @ jnp.diag(y_train)\n",
    "    opt = NysADMM(fun=fun, \n",
    "              grad_fun=grad_fun, \n",
    "              hvp_fun=hvp_fun, \n",
    "              reg_g=reg_g, \n",
    "              prox_reg_h=prox_reg_h, \n",
    "              step_size=step_size, \n",
    "              sketch_size=sketch_size, \n",
    "              maxiter=maxiter)\n",
    "    params, state = opt.run(init_params, G, prox_reg_h_params={'C': C})\n",
    "\n",
    "# use same hyerparameters as NysADMM\n",
    "def solve_with_SVC(): \n",
    "    clf = SVC(C=1.0, gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"NysADMM:\")\n",
    "%timeit solve_with_NysADMM()\n",
    "\n",
    "print(\"SVC:\")\n",
    "%timeit solve_with_SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa4dfd-afd0-43cc-91f9-2877378e6d0e",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":labelprefix: SVM\n",
    ":keyprefix: svm-\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd401542-5f2e-40de-946a-1bfd7d157a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
