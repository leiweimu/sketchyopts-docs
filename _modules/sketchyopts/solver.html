
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sketchyopts.solver &#8212; SketchyOpts  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=2e2ce55c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../../_static/katex_autorenderer.js?v=bd6cae8e"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/sketchyopts/solver';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">SketchyOpts  documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../api/sketchyopts.preconditioner.html">Preconditioners</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/sketchyopts.solver.html">Solvers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/sketchyopts.base.html">Base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/sketchyopts.util.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/sketchyopts.error.html">Error Messages</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/udellgroup/sketchyopts" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for sketchyopts.solver</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">Array</span>
<span class="kn">from</span> <span class="nn">jax.typing</span> <span class="kn">import</span> <span class="n">ArrayLike</span>

<span class="kn">from</span> <span class="nn">sketchyopts.base</span> <span class="kn">import</span> <span class="n">PromiseSolver</span><span class="p">,</span> <span class="n">SolverState</span>
<span class="kn">from</span> <span class="nn">sketchyopts.errors</span> <span class="kn">import</span> <span class="n">InputDimError</span><span class="p">,</span> <span class="n">MatrixNotSquareError</span>
<span class="kn">from</span> <span class="nn">sketchyopts.preconditioner</span> <span class="kn">import</span> <span class="n">rand_nystrom_approx</span>
<span class="kn">from</span> <span class="nn">sketchyopts.util</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ravel_tree</span><span class="p">,</span>
    <span class="n">tree_add</span><span class="p">,</span>
    <span class="n">tree_add_scalar_mul</span><span class="p">,</span>
    <span class="n">tree_l2_norm</span><span class="p">,</span>
    <span class="n">tree_scalar_mul</span><span class="p">,</span>
    <span class="n">tree_sub</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">KeyArray</span> <span class="o">=</span> <span class="n">Array</span>
<span class="n">KeyArrayLike</span> <span class="o">=</span> <span class="n">ArrayLike</span>


<div class="viewcode-block" id="nystrom_pcg">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.nystrom_pcg">[docs]</a>
<span class="k">def</span> <span class="nf">nystrom_pcg</span><span class="p">(</span>
    <span class="n">A</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span>
    <span class="n">mu</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">key</span><span class="p">:</span> <span class="n">KeyArrayLike</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">x0</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ArrayLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">maxiter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">Array</span><span class="p">,</span> <span class="n">Array</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Nyström preconditioned conjugate gradient method (Nyström PCG).</span>

<span class="sd">    The function solves the regularized linear system :math:`(A + \mu I)x = b` using</span>
<span class="sd">    Nyström PCG.</span>

<span class="sd">    Nyström PCG uses randomized Nyström preconditioner by implicitly applying</span>

<span class="sd">    .. math::</span>
<span class="sd">        P^{-1} = (\hat{\lambda}_{l} + \mu) U (\hat{\Lambda} + \mu I)^{-1} U^{T} + (I - U U^{T})</span>

<span class="sd">    where :math:`U` and :math:`\hat{\Lambda}` are from rank-:math:`l` randomized Nyström</span>
<span class="sd">    approximation (here :math:`\hat{\lambda}_{l}` is the :math:`l`:sup:`th` diagonal</span>
<span class="sd">    entry of :math:`\hat{\Lambda}`).</span>

<span class="sd">    Nyström PCG terminates if the :math:`\ell_2`-norm of the residual</span>
<span class="sd">    :math:`b - (A + \mu I)\hat{x}` is within the specified tolerance or it has reached</span>
<span class="sd">    the maximal number of iterations.</span>

<span class="sd">    References:</span>
<span class="sd">      - Z\. Frangella, J. A. Tropp, and M. Udell, `Randomized Nyström preconditioning &lt;https://epubs.siam.org/doi/10.1137/21M1466244&gt;`_. SIAM Journal on Matrix Analysis and Applications, vol. 44, iss. 2, 2023, pp. 718-752.</span>

<span class="sd">    Args:</span>
<span class="sd">      A: A two-dimensional array representing a positive-semidefinite matrix.</span>
<span class="sd">      b: A vector or a two-dimensional array giving the righthand side(s) of the</span>
<span class="sd">        regularized linear system.</span>
<span class="sd">      mu: Regularization parameter. Expect a non-negative value.</span>
<span class="sd">      rank: Rank of the randomized Nyström approximation (which coincides with sketch</span>
<span class="sd">        size). Expect a positive value.</span>
<span class="sd">      key: A PRNG key used as the random key.</span>
<span class="sd">      x0: Initial guess for the solution (same size as righthand side(s) ``b``; default</span>
<span class="sd">        ``None``). When set to ``None``, the algorithm uses zero vector as starting</span>
<span class="sd">        guess.</span>
<span class="sd">      maxiter: Maximum number of iterations (default ``None``). When set to ``None``,</span>
<span class="sd">        the algorithm only terminates when the specified tolerance has been achieved.</span>
<span class="sd">        Internally the value gets set to :math:`10` times the size of the system.</span>
<span class="sd">      tol: Solution tolerance (default ``1e-5``).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A four-element tuple containing</span>

<span class="sd">      - **x** – Approximate solution to the regularized linear system. Solution has the</span>
<span class="sd">        same size as righthand side(s) ``b``.</span>
<span class="sd">      - **r** – Residual of the approximate solution. Residual has the same size as</span>
<span class="sd">        righthand side(s) ``b``.</span>
<span class="sd">      - **status** – Whether or not the approximate solution has converged for each</span>
<span class="sd">        righthand side. Status has the same size as the number of righthand side(s).</span>
<span class="sd">      - **k** – Total number of iterations to reach to the approximate solution.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># perform randomized Nyström approximation</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">rand_nystrom_approx</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

    <span class="c1"># matrix-vector (or mat-mat for multiple righthand sides) product for regularized</span>
    <span class="c1"># linear operator</span>
    <span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">regularized_A</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">A</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">x</span>

    <span class="c1"># matrix-vector (or mat-mat for multiple righthand sides) product for inverse</span>
    <span class="c1"># Nyström preconditioner</span>
    <span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">inv_preconditioner</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">UTx</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">x</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">S</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">U</span> <span class="o">@</span> <span class="p">(</span><span class="n">UTx</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">S</span> <span class="o">+</span> <span class="n">mu</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span> <span class="o">-</span> <span class="n">U</span> <span class="o">@</span> <span class="n">UTx</span>

    <span class="c1"># condition evaluation</span>
    <span class="k">def</span> <span class="nf">cond_fun</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">k</span> <span class="o">&lt;</span> <span class="n">maxiter</span><span class="p">)</span>

    <span class="c1"># PCG iteration</span>
    <span class="k">def</span> <span class="nf">body_fun</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">value</span>
        <span class="c1"># select only columns corresponding to the righthand side that has yet converged</span>
        <span class="c1"># populate the remaining columns with NaN</span>
        <span class="n">xs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">[:,],</span> <span class="n">jnp</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
        <span class="n">rs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">r</span><span class="p">[:,],</span> <span class="n">jnp</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
        <span class="n">zs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">z</span><span class="p">[:,],</span> <span class="n">jnp</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
        <span class="n">ps</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="p">[:,],</span> <span class="n">jnp</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
        <span class="c1"># perform update on selected columns and ignore padded columns</span>
        <span class="c1"># (i.e. with NaN values)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">regularized_A</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">rs</span> <span class="o">*</span> <span class="n">zs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ps</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">x_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">xs</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">ps</span><span class="p">)[:,],</span> <span class="n">x</span><span class="p">[:,])</span>
        <span class="n">r_s</span> <span class="o">=</span> <span class="n">rs</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">v</span>
        <span class="n">r_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">r_s</span><span class="p">[:,],</span> <span class="n">r</span><span class="p">[:,])</span>
        <span class="n">z_s</span> <span class="o">=</span> <span class="n">inv_preconditioner</span><span class="p">(</span><span class="n">r_s</span><span class="p">)</span>
        <span class="n">z_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">z_s</span><span class="p">[:,],</span> <span class="n">z</span><span class="p">[:,])</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">r_s</span> <span class="o">*</span> <span class="n">z_s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="n">gamma</span>
        <span class="n">p_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">z_s</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">ps</span><span class="p">)[:,],</span> <span class="n">p</span><span class="p">[:,])</span>
        <span class="n">r_norm</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">r_s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">mask_</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">r_norm</span> <span class="o">&gt;</span> <span class="n">tol</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># NaN always evaluates to False</span>
        <span class="k">return</span> <span class="n">x_</span><span class="p">,</span> <span class="n">r_</span><span class="p">,</span> <span class="n">z_</span><span class="p">,</span> <span class="n">p_</span><span class="p">,</span> <span class="n">mask_</span><span class="p">,</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="c1"># dimension check</span>
    <span class="k">if</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">InputDimError</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">jnp</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="n">MatrixNotSquareError</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="n">InputDimError</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

    <span class="c1"># initialization</span>
    <span class="n">b_ndim</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">maxiter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">maxiter</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>  <span class="c1"># same behavior as SciPy</span>

    <span class="c1"># initial step</span>
    <span class="n">r0</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">regularized_A</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">p0</span> <span class="o">=</span> <span class="n">z0</span> <span class="o">=</span> <span class="n">inv_preconditioner</span><span class="p">(</span><span class="n">r0</span><span class="p">)</span>
    <span class="n">mask0</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">b</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">initial_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">r0</span><span class="p">,</span> <span class="n">z0</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">mask0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">x_final</span><span class="p">,</span> <span class="n">r_final</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">mask_final</span><span class="p">,</span> <span class="n">k_final</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
        <span class="n">cond_fun</span><span class="p">,</span> <span class="n">body_fun</span><span class="p">,</span> <span class="n">initial_value</span>
    <span class="p">)</span>

    <span class="c1"># match solution and residual to the input shape if b has a single dimension</span>
    <span class="k">if</span> <span class="n">b_ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">x_final</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x_final</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="n">r_final</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">r_final</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x_final</span><span class="p">,</span> <span class="n">r_final</span><span class="p">,</span> <span class="o">~</span><span class="n">mask_final</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">),</span> <span class="n">k_final</span>  <span class="c1"># type: ignore</span></div>



<div class="viewcode-block" id="SketchySGDState">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchySGDState">[docs]</a>
<span class="k">class</span> <span class="nc">SketchySGDState</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The SketchySGD optimizer state.</span>

<span class="sd">    Args:</span>
<span class="sd">      iter_num: Number of iterations the optimizer has performed.</span>
<span class="sd">      value: Objective value at the current iterate.</span>
<span class="sd">      error: Gradient norm of the current iterate.</span>
<span class="sd">      key: PRNG key for the next update.</span>
<span class="sd">      precond: Preconditioner for the next update.</span>
<span class="sd">      step_size: Step size for the next update.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">iter_num</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">error</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">key</span><span class="p">:</span> <span class="n">KeyArray</span>
    <span class="n">precond</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">step_size</span><span class="p">:</span> <span class="n">Array</span></div>



<div class="viewcode-block" id="SketchySGD">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchySGD">[docs]</a>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">eq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kw_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SketchySGD</span><span class="p">(</span><span class="n">PromiseSolver</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The SketchySGD optimizer.</span>

<span class="sd">    SketchySGD is a stochastic quasi-Newton method that uses sketching to approximate the</span>
<span class="sd">    curvature of the loss function. It maintains a preconditioner for SGD (stochastic gradient</span>
<span class="sd">    descent) using randomized low-rank Nyström approximations to the subsampled Hessian and</span>
<span class="sd">    automatically selects an appropriate learning whenever it updates the preconditioner.</span>

<span class="sd">    Example:</span>
<span class="sd">      .. highlight:: python</span>
<span class="sd">      .. code-block:: python</span>

<span class="sd">        import jax.numpy as jnp</span>
<span class="sd">        from sketchyopts.solver import SketchySGD</span>

<span class="sd">        def ridge_reg_objective(params, l2reg, data):</span>
<span class="sd">            # data has dimension num_samples * (feature_dim + 1)</span>
<span class="sd">            X, y = data[:,:feature_dim], data[:,feature_dim:]</span>
<span class="sd">            residuals = jnp.dot(X, params) - y</span>
<span class="sd">            return jnp.mean(residuals ** 2) + 0.5 * l2reg * jnp.dot(w ** 2)</span>

<span class="sd">        opt = SketchySGD(ridge_reg_objective, ...)</span>
<span class="sd">        opt.run(init_params, data, l2reg=l2reg)</span>

<span class="sd">    References:</span>
<span class="sd">      - Z\. Frangella, P. Rathore, S. Zhao, and M. Udell, `SketchySGD: Reliable Stochastic Optimization via Randomized Curvature Estimates &lt;https://arxiv.org/abs/2211.08597&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">      fun: Scalar-valued objective function. The function needs to have the optimization variable as its first argument, and data input argument ``data``. For example, the function signature might take the form of ``fun(params, some_arg, data, other_arg)``.</span>
<span class="sd">      grad_fun: Optional gradient oracle corresponding to the provided objective function ``fun``. The gradient function returns the gradient of the objective function with respect to its first argument (*i.e.* the optimization variable). The gradient function should have the same function signature as the objective function, and the returned gradient should have the same shapes and types as the the optimization variable (*i.e.* the first argument of ``fun``).</span>
<span class="sd">      hvp_fun: Optional Hessian-vector product oracle. The hvp function should take optimization variable and vector (of the same shape and type) as its first and second arguments, respectively. It is also expected to have the same additional arguments as the provided objective function ``fun``. The returned result should have the same shapes and types as the the optimization variable (*i.e.* the first argument of ``fun``).</span>
<span class="sd">      pre_update: Optional function to execute before optimizer&#39;s each update on the iterate. The function expects signature ``params, state = pre_update(params, state, *args, **kwargs)`` where ``state`` is the :class:`SketchySGDState` object.</span>
<span class="sd">      rho: Regularization parameter. Expect a non-negative value.</span>
<span class="sd">      rank: Rank of the preconditioner. Expect a positive value.</span>
<span class="sd">      grad_batch_size: Size of the batch of data to compute stochastic gradient at each iteration. Expect a positive value.</span>
<span class="sd">      hess_batch_size: Size of the batch of data to estimate the stochastic Hessian when updating the preconditioner. Expect a positive value.</span>
<span class="sd">      update_freq: Update frequency of the preconditioner. When set to ``0`` or :math:`\infty` (*e.g.* ``jax.numpy.inf`` or</span>
<span class="sd">        ``numpy.inf``), the optimizer uses constant preconditioner that is constructed at</span>
<span class="sd">        the beginning of the optimization process.</span>
<span class="sd">      seed: Initial seed for the random number generator.</span>
<span class="sd">      learning_rate: Step size for applying updates (default ``0.5``). It can either be</span>
<span class="sd">        a fixed scalar value or a schedule based on step count. If a fixed scalar value is</span>
<span class="sd">        provided, the optimizer uses the value as the multiplier to the adaptively chosen</span>
<span class="sd">        learning rate whenever the preconditioner is updated. If a schedule (in the form of a</span>
<span class="sd">        function) is provided, the optimizer solely relies on the schedule and no longer</span>
<span class="sd">        computes and determines the learning rate adaptively.</span>
<span class="sd">      maxiter: Maximum number of iterations to run the optimizer (default ``20``). Expect a positive value.</span>
<span class="sd">      tol: Threshold of the gradient norm used for terminating the optimizer (default ``1e-3``).</span>
<span class="sd">      verbose: Whether to print diagnostic message (default ``false``).</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">learning_rate</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span>

    <span class="k">def</span> <span class="nf">_init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SketchySGDState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function initializes the optimizer state.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">SketchySGDState</span><span class="p">(</span>
            <span class="n">iter_num</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">value</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">),</span>
            <span class="n">error</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">),</span>
            <span class="n">key</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">),</span>
            <span class="n">precond</span><span class="o">=</span><span class="k">lambda</span> <span class="n">g</span><span class="p">:</span> <span class="n">g</span><span class="p">,</span>
            <span class="n">step_size</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SolverState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function performs an update on the iterate.&quot;&quot;&quot;</span>
        <span class="c1"># generate a random batch</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">key</span><span class="p">)</span>
        <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="n">subkey</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_batch_size</span><span class="p">,),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="c1"># call custom pre-update function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_update</span><span class="p">:</span>
            <span class="n">params</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_update</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># compute preconditioned stochastic gradient</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">tree_l2_norm</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">unraveled_grad</span><span class="p">,</span> <span class="n">unravel_fun</span> <span class="o">=</span> <span class="n">ravel_tree</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
        <span class="n">direction</span> <span class="o">=</span> <span class="n">unravel_fun</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">precond</span><span class="p">(</span><span class="n">unraveled_grad</span><span class="p">))</span>

        <span class="c1"># perform an update</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">tree_add_scalar_mul</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">-</span><span class="n">state</span><span class="o">.</span><span class="n">step_size</span><span class="p">,</span> <span class="n">direction</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">SolverState</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
            <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span>
                <span class="n">iter_num</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="n">error</span><span class="p">,</span>
                <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

<div class="viewcode-block" id="SketchySGD.run">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchySGD.run">[docs]</a>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init_params</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SolverState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function runs the optimization loop.</span>

<span class="sd">        Args:</span>
<span class="sd">          init_params: Initial value of the optimization variable.</span>
<span class="sd">          data: Full dataset. Expect an array of shape ``(num_samples, ...)``.</span>
<span class="sd">          *args: Additional positional arguments to be passed to ``fun`` (and ``grad_fun`` as well as ``hvp_fun`` if provided).</span>
<span class="sd">          **kwargs: Additional keyword arguments to be passed to ``fun`` (and ``grad_fun`` as well as ``hvp_fun`` if provided).</span>
<span class="sd">        Returns:</span>
<span class="sd">          Final optimization variable and solver state. The variable has the same shape as the provided initial value ``init_params``, and the state is an :class:`SketchySGDState` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># initialize iterate and state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">init_params</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_state</span><span class="p">()</span>

        <span class="c1"># jit the parameter update function</span>
        <span class="n">update_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_params</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># run the optimization loop</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">maxiter</span><span class="p">):</span>

            <span class="c1"># update preconditioner</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_precond</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="c1"># update iterate</span>
            <span class="n">params</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">update_params</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

            <span class="c1"># break out of loop if tolerance has been reached</span>
            <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">error</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                <span class="n">jax</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">print</span><span class="p">(</span>
                    <span class="s2">&quot;Info: early termination because error tolerance has been reached.&quot;</span>
                <span class="p">)</span>
                <span class="k">break</span>

        <span class="k">return</span> <span class="n">SolverState</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="SketchySVRGState">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchySVRGState">[docs]</a>
<span class="k">class</span> <span class="nc">SketchySVRGState</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The SketchySVRG optimizer state.</span>

<span class="sd">    Args:</span>
<span class="sd">      iter_num: Number of iterations the optimizer has performed.</span>
<span class="sd">      value: Objective value at the current iterate.</span>
<span class="sd">      error: Gradient norm of the current iterate.</span>
<span class="sd">      key: PRNG key for the next update.</span>
<span class="sd">      precond: Preconditioner for the next update.</span>
<span class="sd">      full_grad: Full gradient at the snapshot.</span>
<span class="sd">      snapshot: Snapshot of the iterate.</span>
<span class="sd">      step_size: Step size for the next update.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">iter_num</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">error</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">key</span><span class="p">:</span> <span class="n">KeyArray</span>
    <span class="n">precond</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">full_grad</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">snapshot</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">step_size</span><span class="p">:</span> <span class="n">Array</span></div>



<div class="viewcode-block" id="SketchySVRG">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchySVRG">[docs]</a>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">eq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kw_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SketchySVRG</span><span class="p">(</span><span class="n">PromiseSolver</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The SketchySVRG optimizer.</span>

<span class="sd">    SketchySVRG is a preconditioned version of SVRG [#f1]_ (stochastic variance reduced gradient).</span>
<span class="sd">    The optimizer stores and periodically updates a snapshot of the iterate, and uses the snapshot</span>
<span class="sd">    and its full gradient to perform variance reduction. The preconditioning is then applied to</span>
<span class="sd">    the variance-reduced stochastic gradient at each step. The preconditioner and learning rate get</span>
<span class="sd">    updated periodically.</span>

<span class="sd">    Example:</span>
<span class="sd">      .. highlight:: python</span>
<span class="sd">      .. code-block:: python</span>

<span class="sd">        import jax.numpy as jnp</span>
<span class="sd">        from sketchyopts.solver import SketchySVRG</span>

<span class="sd">        def ridge_reg_objective(params, l2reg, data):</span>
<span class="sd">            # data has dimension num_samples * (feature_dim + 1)</span>
<span class="sd">            X, y = data[:,:feature_dim], data[:,feature_dim:]</span>
<span class="sd">            residuals = jnp.dot(X, params) - y</span>
<span class="sd">            return jnp.mean(residuals ** 2) + 0.5 * l2reg * jnp.dot(w ** 2)</span>

<span class="sd">        opt = SketchySVRG(ridge_reg_objective, ...)</span>
<span class="sd">        opt.run(init_params, data, l2reg=l2reg)</span>

<span class="sd">    .. rubric:: References</span>

<span class="sd">    .. [#f1] R\. Johnson and T. Zhang, `Accelerating Stochastic Gradient Descent using Predictive Variance Reduction &lt;https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction&gt;`_, Advances in Neural Information Processing Systems, 26, 2013.</span>

<span class="sd">    - Z\. Frangella, P. Rathore, S. Zhao, and M. Udell, `PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates &lt;https://arxiv.org/abs/2309.02014&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">      fun: Scalar-valued objective function. The function needs to have the optimization variable as its first argument, and data input argument ``data``. For example, the function signature might take the form of ``fun(params, some_arg, data, other_arg)``.</span>
<span class="sd">      grad_fun: Optional gradient oracle corresponding to the provided objective function ``fun``. The gradient function returns the gradient of the objective function with respect to its first argument (*i.e.* the optimization variable). The gradient function should have the same function signature as the objective function, and the returned gradient should have the same shapes and types as the the optimization variable (*i.e.* the first argument of ``fun``).</span>
<span class="sd">      hvp_fun: Optional Hessian-vector product oracle. The hvp function should take optimization variable and vector (of the same shape and type) as its first and second arguments, respectively. It is also expected to have the same additional arguments as the provided objective function ``fun``. The returned result should have the same shapes and types as the the optimization variable (*i.e.* the first argument of ``fun``).</span>
<span class="sd">      pre_update: Optional function to execute before optimizer&#39;s each update on the iterate. The function expects signature ``params, state = pre_update(params, state, *args, **kwargs)`` where ``state`` is the :class:`SketchySVRGState` object.</span>
<span class="sd">      rho: Regularization parameter. Expect a non-negative value.</span>
<span class="sd">      rank: Rank of the preconditioner. Expect a positive value.</span>
<span class="sd">      grad_batch_size: Size of the batch of data to compute stochastic gradient at each iteration. Expect a positive value.</span>
<span class="sd">      hess_batch_size: Size of the batch of data to estimate the stochastic Hessian when updating the preconditioner. Expect a positive value.</span>
<span class="sd">      update_freq: Update frequency of the preconditioner. When set to ``0`` or :math:`\infty` (*e.g.* ``jax.numpy.inf`` or</span>
<span class="sd">        ``numpy.inf``), the optimizer uses constant preconditioner that is constructed at</span>
<span class="sd">        the beginning of the optimization process.</span>
<span class="sd">      snapshop_update_freq: Update frequency of the snapshot. Expect a positive value.</span>
<span class="sd">      seed: Initial seed for the random number generator.</span>
<span class="sd">      learning_rate: Step size for applying updates (default ``0.5``). It can either be</span>
<span class="sd">        a fixed scalar value or a schedule based on step count. If a fixed scalar value is</span>
<span class="sd">        provided, the optimizer uses the value as the multiplier to the adaptively chosen</span>
<span class="sd">        learning rate whenever the preconditioner is updated. If a schedule (in the form of a</span>
<span class="sd">        function) is provided, the optimizer solely relies on the schedule and no longer</span>
<span class="sd">        computes and determines the learning rate adaptively.</span>
<span class="sd">      maxiter: Maximum number of iterations to run the optimizer (default ``20``). Expect a positive value.</span>
<span class="sd">      tol: Threshold of the gradient norm used for terminating the optimizer (default ``1e-3``).</span>
<span class="sd">      verbose: Whether to print diagnostic message (default ``false``).</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">snapshop_update_freq</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span>

    <span class="k">def</span> <span class="nf">_init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SketchySVRGState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function initializes the optimizer state.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">SketchySVRGState</span><span class="p">(</span>
            <span class="n">iter_num</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">value</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">),</span>
            <span class="n">error</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">),</span>
            <span class="n">key</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">),</span>
            <span class="n">precond</span><span class="o">=</span><span class="k">lambda</span> <span class="n">g</span><span class="p">:</span> <span class="n">g</span><span class="p">,</span>
            <span class="n">full_grad</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">snapshot</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">step_size</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_snapshot</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SketchySVRGState</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">full_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
        <span class="n">unraveled_full_grad</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ravel_tree</span><span class="p">(</span><span class="n">full_grad</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span>
            <span class="n">full_grad</span><span class="o">=</span><span class="n">unraveled_full_grad</span><span class="p">,</span>
            <span class="n">snapshot</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SolverState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function performs an update on the iterate.&quot;&quot;&quot;</span>
        <span class="c1"># generate a random batch</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">key</span><span class="p">)</span>
        <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="n">subkey</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_batch_size</span><span class="p">,),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="c1"># call custom pre-update function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_update</span><span class="p">:</span>
            <span class="n">params</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_update</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># compute preconditioned variance-reduced stochastic gradient</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">grad_snapshot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad</span><span class="p">(</span>
            <span class="n">state</span><span class="o">.</span><span class="n">snapshot</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">error</span> <span class="o">=</span> <span class="n">tree_l2_norm</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">unraveled_grad</span><span class="p">,</span> <span class="n">unravel_fun</span> <span class="o">=</span> <span class="n">ravel_tree</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
        <span class="n">unraveled_grad_snapshot</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ravel_tree</span><span class="p">(</span><span class="n">grad_snapshot</span><span class="p">)</span>

        <span class="n">direction</span> <span class="o">=</span> <span class="n">unravel_fun</span><span class="p">(</span>
            <span class="n">state</span><span class="o">.</span><span class="n">precond</span><span class="p">(</span><span class="n">unraveled_grad</span> <span class="o">-</span> <span class="n">unraveled_grad_snapshot</span> <span class="o">+</span> <span class="n">state</span><span class="o">.</span><span class="n">full_grad</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># perform an update</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">tree_add_scalar_mul</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">-</span><span class="n">state</span><span class="o">.</span><span class="n">step_size</span><span class="p">,</span> <span class="n">direction</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">SolverState</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
            <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span>
                <span class="n">iter_num</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="n">error</span><span class="p">,</span>
                <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

<div class="viewcode-block" id="SketchySVRG.run">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchySVRG.run">[docs]</a>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init_params</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SolverState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function runs the optimization loop.</span>

<span class="sd">        Args:</span>
<span class="sd">          init_params: Initial value of the optimization variable.</span>
<span class="sd">          data: Full dataset. Expect an array of shape ``(num_samples, ...)``.</span>
<span class="sd">          *args: Additional positional arguments to be passed to ``fun`` (and ``grad_fun`` as well as ``hvp_fun`` if provided).</span>
<span class="sd">          **kwargs: Additional keyword arguments to be passed to ``fun`` (and ``grad_fun`` as well as ``hvp_fun`` if provided).</span>
<span class="sd">        Returns:</span>
<span class="sd">          Final optimization variable and solver state. The variable has the same shape as the provided initial value ``init_params``, and the state is an :class:`SketchySVRGState` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># initialize iterate and state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">init_params</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_state</span><span class="p">()</span>

        <span class="c1"># jit the parameter update function</span>
        <span class="n">update_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_params</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># run the optimization loop</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">maxiter</span><span class="p">):</span>

            <span class="c1"># update snapshot</span>
            <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">snapshop_update_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_snapshot</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="c1"># update preconditioner</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_precond</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="c1"># update iterate</span>
            <span class="n">params</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">update_params</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

            <span class="c1"># break out of loop if tolerance has been reached</span>
            <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">error</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                <span class="n">jax</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">print</span><span class="p">(</span>
                    <span class="s2">&quot;Info: early termination because error tolerance has been reached.&quot;</span>
                <span class="p">)</span>
                <span class="k">break</span>

        <span class="k">return</span> <span class="n">SolverState</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="SketchySAGAState">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchySAGAState">[docs]</a>
<span class="k">class</span> <span class="nc">SketchySAGAState</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The SketchySAGA optimizer state.</span>

<span class="sd">    Args:</span>
<span class="sd">      iter_num: Number of iterations the optimizer has performed.</span>
<span class="sd">      value: Objective value at the current iterate.</span>
<span class="sd">      error: Gradient norm of the current iterate.</span>
<span class="sd">      key: PRNG key for the next update.</span>
<span class="sd">      precond: Preconditioner for the next update.</span>
<span class="sd">      grad_table: Table of gradients of each individual component.</span>
<span class="sd">      table_avg: Average of the gradients in the table.</span>
<span class="sd">      step_size: Step size for the next update.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">iter_num</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">error</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">key</span><span class="p">:</span> <span class="n">KeyArray</span>
    <span class="n">precond</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">grad_table</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">table_avg</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">step_size</span><span class="p">:</span> <span class="n">Array</span></div>



<div class="viewcode-block" id="SketchySAGA">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchySAGA">[docs]</a>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">eq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kw_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SketchySAGA</span><span class="p">(</span><span class="n">PromiseSolver</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The SketchySAGA optimizer.</span>

<span class="sd">    SketchySAGA is a preconditioned version of a minibatch variant [#f3]_ (b-nice SAGA) of SAGA [#f2]_.</span>
<span class="sd">    The optimizer maintains and updates a gradient table and table average. At each iteration,</span>
<span class="sd">    the optimizer computes auxiliary vector and uses it to update the variance-reduced stochastic</span>
<span class="sd">    gradient. The update is then based on the preconditioned variance-reduced stochastic gradient.</span>

<span class="sd">    .. note:: Because SketchySAGA computes gradient of each individual component function, the provided objective function ``fun`` or gradient function ``grad_fun`` needs to be compatible with 1-dimensional data input (*i.e.* when the data input is a vector representing a single sample). The following example expands the vector to a 2-dimensional array to handle the single sample case.</span>

<span class="sd">    Example:</span>
<span class="sd">      .. highlight:: python</span>
<span class="sd">      .. code-block:: python</span>

<span class="sd">        import jax.numpy as jnp</span>
<span class="sd">        from sketchyopts.solver import SketchySAGA</span>

<span class="sd">        def ridge_reg_objective(params, l2reg, data):</span>
<span class="sd">            # make data 2-dimensional if it is a vector of a single sample</span>
<span class="sd">            if jnp.ndim(data) == 1:</span>
<span class="sd">                data = jnp.expand_dims(data, axis=0)</span>
<span class="sd">            # data has dimension num_samples * (feature_dim + 1)</span>
<span class="sd">            X, y = data[:,:feature_dim], data[:,feature_dim:]</span>
<span class="sd">            residuals = jnp.dot(X, params) - y</span>
<span class="sd">            return jnp.mean(residuals ** 2) + 0.5 * l2reg * jnp.dot(w ** 2)</span>

<span class="sd">        opt = SketchySAGA(ridge_reg_objective, ...)</span>
<span class="sd">        opt.run(init_params, data, l2reg=l2reg)</span>

<span class="sd">    .. rubric:: References</span>

<span class="sd">    .. [#f2] A\. Defazio, F. Bach, and S. Lacoste-Julien, `SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives &lt;https://papers.nips.cc/paper_files/paper/2014/hash/ede7e2b6d13a41ddf9f4bdef84fdc737-Abstract.html&gt;`_, Advances in Neural Information Processing Systems, 27, 2014.</span>
<span class="sd">    .. [#f3] N\. Gazagnadou, R. M. Gower, and J. Salmon, `Optimal Mini-Batch and Step Sizes for SAGA &lt;https://proceedings.mlr.press/v97/gazagnadou19a.html&gt;`_, in *Proceedings of the 36*\ :sup:`th` *International Conference on Machine Learning*, Proceedings of Machine Learning Research (PMLR), 97: 2142-2150, 2019.</span>

<span class="sd">    - Z\. Frangella, P. Rathore, S. Zhao, and M. Udell, `PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates &lt;https://arxiv.org/abs/2309.02014&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">      fun: Scalar-valued objective function. The function needs to have the optimization variable as its first argument, and data input argument ``data``. For example, the function signature might take the form of ``fun(params, some_arg, data, other_arg)``.</span>
<span class="sd">      grad_fun: Optional gradient oracle corresponding to the provided objective function ``fun``. The gradient function returns the gradient of the objective function with respect to its first argument (*i.e.* the optimization variable). The gradient function should have the same function signature as the objective function, and the returned gradient should have the same shapes and types as the the optimization variable (*i.e.* the first argument of ``fun``).</span>
<span class="sd">      hvp_fun: Optional Hessian-vector product oracle. The hvp function should take optimization variable and vector (of the same shape and type) as its first and second arguments, respectively. It is also expected to have the same additional arguments as the provided objective function ``fun``. The returned result should have the same shapes and types as the the optimization variable (*i.e.* the first argument of ``fun``).</span>
<span class="sd">      pre_update: Optional function to execute before optimizer&#39;s each update on the iterate. The function expects signature ``params, state = pre_update(params, state, *args, **kwargs)`` where ``state`` is the :class:`SketchySAGAState` object.</span>
<span class="sd">      rho: Regularization parameter. Expect a non-negative value.</span>
<span class="sd">      rank: Rank of the preconditioner. Expect a positive value.</span>
<span class="sd">      grad_batch_size: Size of the batch of data to compute stochastic gradient at each iteration. Expect a positive value.</span>
<span class="sd">      hess_batch_size: Size of the batch of data to estimate the stochastic Hessian when updating the preconditioner. Expect a positive value.</span>
<span class="sd">      update_freq: Update frequency of the preconditioner. When set to ``0`` or :math:`\infty` (*e.g.* ``jax.numpy.inf`` or</span>
<span class="sd">        ``numpy.inf``), the optimizer uses constant preconditioner that is constructed at</span>
<span class="sd">        the beginning of the optimization process.</span>
<span class="sd">      seed: Initial seed for the random number generator.</span>
<span class="sd">      learning_rate: Step size for applying updates (default ``0.5``). It can either be</span>
<span class="sd">        a fixed scalar value or a schedule based on step count. If a fixed scalar value is</span>
<span class="sd">        provided, the optimizer uses the value as the multiplier to the adaptively chosen</span>
<span class="sd">        learning rate whenever the preconditioner is updated. If a schedule (in the form of a</span>
<span class="sd">        function) is provided, the optimizer solely relies on the schedule and no longer</span>
<span class="sd">        computes and determines the learning rate adaptively.</span>
<span class="sd">      maxiter: Maximum number of iterations to run the optimizer (default ``20``). Expect a positive value.</span>
<span class="sd">      tol: Threshold of the gradient norm used for terminating the optimizer (default ``1e-3``).</span>
<span class="sd">      verbose: Whether to print diagnostic message (default ``false``).</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">learning_rate</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function overrides the superclass&#39;s method and constructs`` value_and_grad`` function that computes the gradient of each individual component.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_fun</span><span class="p">):</span>
            <span class="n">comp_grad_fun</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fun</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">comp_grad_fun</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fun</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">value_and_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">g</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">ravel_tree</span><span class="p">(</span><span class="n">comp_grad_fun</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">d</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">grad_fun</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fun</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span> <span class="n">grad_fun</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad</span> <span class="o">=</span> <span class="n">value_and_grad</span>

    <span class="k">def</span> <span class="nf">_init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SketchySAGAState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function initializes the optimizer state.&quot;&quot;&quot;</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">p</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ravel_tree</span><span class="p">(</span><span class="n">params</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">SketchySAGAState</span><span class="p">(</span>
            <span class="n">iter_num</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">value</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">),</span>
            <span class="n">error</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">),</span>
            <span class="n">key</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">),</span>
            <span class="n">precond</span><span class="o">=</span><span class="k">lambda</span> <span class="n">g</span><span class="p">:</span> <span class="n">g</span><span class="p">,</span>
            <span class="n">grad_table</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)),</span>
            <span class="n">table_avg</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">p</span><span class="p">),</span>
            <span class="n">step_size</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SolverState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function performs an update on the iterate.&quot;&quot;&quot;</span>
        <span class="c1"># generate a random batch</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">key</span><span class="p">)</span>
        <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="n">subkey</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_batch_size</span><span class="p">,),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="c1"># call custom pre-update function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_update</span><span class="p">:</span>
            <span class="n">params</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_update</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># compute stochastic gradients</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">unraveled_grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">unraveled_grads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

        <span class="c1"># compute auxiliary</span>
        <span class="n">aux</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">unraveled_grads</span> <span class="o">-</span> <span class="n">state</span><span class="o">.</span><span class="n">grad_table</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># compute preconditioned variance-reduced stochastic gradient</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">unravel_fun</span> <span class="o">=</span> <span class="n">ravel_tree</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">direction</span> <span class="o">=</span> <span class="n">unravel_fun</span><span class="p">(</span>
            <span class="n">state</span><span class="o">.</span><span class="n">precond</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">table_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">aux</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># update table average and gradient table</span>
        <span class="n">table_avg</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">table_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">aux</span>
        <span class="n">grad_table</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">grad_table</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">unraveled_grads</span><span class="p">)</span>

        <span class="c1"># perform an update</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">tree_add_scalar_mul</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">-</span><span class="n">state</span><span class="o">.</span><span class="n">step_size</span><span class="p">,</span> <span class="n">direction</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">SolverState</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
            <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span>
                <span class="n">iter_num</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="n">error</span><span class="p">,</span>
                <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
                <span class="n">grad_table</span><span class="o">=</span><span class="n">grad_table</span><span class="p">,</span>
                <span class="n">table_avg</span><span class="o">=</span><span class="n">table_avg</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

<div class="viewcode-block" id="SketchySAGA.run">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchySAGA.run">[docs]</a>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init_params</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SolverState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function runs the optimization loop.</span>

<span class="sd">        Args:</span>
<span class="sd">          init_params: Initial value of the optimization variable.</span>
<span class="sd">          data: Full dataset. Expect an array of shape ``(num_samples, ...)``.</span>
<span class="sd">          *args: Additional positional arguments to be passed to ``fun`` (and ``grad_fun`` as well as ``hvp_fun`` if provided).</span>
<span class="sd">          **kwargs: Additional keyword arguments to be passed to ``fun`` (and ``grad_fun`` as well as ``hvp_fun`` if provided).</span>
<span class="sd">        Returns:</span>
<span class="sd">          Final optimization variable and solver state. The variable has the same shape as the provided initial value ``init_params``, and the state is an :class:`SketchySAGAState` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># initialize iterate and state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">init_params</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_state</span><span class="p">(</span><span class="n">init_params</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

        <span class="c1"># jit the parameter update function</span>
        <span class="n">update_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_params</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># run the optimization loop</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">maxiter</span><span class="p">):</span>

            <span class="c1"># update preconditioner</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_precond</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="c1"># update iterate</span>
            <span class="n">params</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">update_params</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

            <span class="c1"># break out of loop if tolerance has been reached</span>
            <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">error</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                <span class="n">jax</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">print</span><span class="p">(</span>
                    <span class="s2">&quot;Info: early termination because error tolerance has been reached.&quot;</span>
                <span class="p">)</span>
                <span class="k">break</span>

        <span class="k">return</span> <span class="n">SolverState</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="SketchyKatyushaState">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchyKatyushaState">[docs]</a>
<span class="k">class</span> <span class="nc">SketchyKatyushaState</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The SketchyKatyusha optimizer state.</span>

<span class="sd">    Args:</span>
<span class="sd">      iter_num: Number of iterations the optimizer has performed.</span>
<span class="sd">      value: Objective value at the current iterate.</span>
<span class="sd">      error: Gradient norm of the current iterate.</span>
<span class="sd">      key: PRNG key for the next update.</span>
<span class="sd">      precond: Preconditioner for the next update.</span>
<span class="sd">      full_grad: Full gradient at the snapshot.</span>
<span class="sd">      snapshot: Snapshot of the iterate.</span>
<span class="sd">      z: Momentum of the iterate.</span>
<span class="sd">      step_size: Step size for the next update.</span>
<span class="sd">      L: Smoothness constant estimate.</span>
<span class="sd">      sigma: Inverse condition number estimate.</span>
<span class="sd">      theta: Momentum parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">iter_num</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">error</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">key</span><span class="p">:</span> <span class="n">KeyArray</span>
    <span class="n">precond</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">full_grad</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">snapshot</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">step_size</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">L</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="n">Array</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Array</span></div>



<div class="viewcode-block" id="SketchyKatyusha">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchyKatyusha">[docs]</a>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">eq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kw_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SketchyKatyusha</span><span class="p">(</span><span class="n">PromiseSolver</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The SketchyKatyusha optimizer.</span>

<span class="sd">    SketchyKatyusha is a preconditioned version of Loopless Katyusha [#f5]_ that extends the original Katyusha [#f4]_.</span>
<span class="sd">    The optimizer calculates the preconditioned variance-reduced stochastic gradient and performs momentum update.</span>
<span class="sd">    The optimizer periodically updates the preconditioner, and probabilistically updates snapshot and full gradient.</span>

<span class="sd">    Example:</span>
<span class="sd">      .. highlight:: python</span>
<span class="sd">      .. code-block:: python</span>

<span class="sd">        import jax.numpy as jnp</span>
<span class="sd">        from sketchyopts.solver import SketchyKatyusha</span>

<span class="sd">        def ridge_reg_objective(params, l2reg, data):</span>
<span class="sd">            # data has dimension num_samples * (feature_dim + 1)</span>
<span class="sd">            X, y = data[:,:feature_dim], data[:,feature_dim:]</span>
<span class="sd">            residuals = jnp.dot(X, params) - y</span>
<span class="sd">            return jnp.mean(residuals ** 2) + 0.5 * l2reg * jnp.dot(w ** 2)</span>

<span class="sd">        opt = SketchyKatyusha(ridge_reg_objective, ...)</span>
<span class="sd">        opt.run(init_params, data, l2reg=l2reg)</span>

<span class="sd">    .. rubric:: References</span>

<span class="sd">    .. [#f4] Z\. Allen-Zhu, `Katyusha: The First Direct Acceleration of Stochastic Gradient Methods &lt;https://jmlr.org/papers/v18/16-410.html&gt;`_, Journal of Machine Learning Research, 18(221): 1-51, 2018.</span>
<span class="sd">    .. [#f5] D\. Kovalev, S. Horváth, and P. Richtárik, `Don&#39;t Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop &lt;https://proceedings.mlr.press/v117/kovalev20a.html&gt;`_, in *Proceedings of the 31*\ :sup:`th` *International Conference on Algorithmic Learning Theory*, Proceedings of Machine Learning Research (PMLR), 117: 451-467, 2020.</span>

<span class="sd">    - Z\. Frangella, P. Rathore, S. Zhao, and M. Udell, `PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates &lt;https://arxiv.org/abs/2309.02014&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">      fun: Scalar-valued objective function. The function needs to have the optimization variable as its first argument, and data input argument ``data``. For example, the function signature might take the form of ``fun(params, some_arg, data, other_arg)``.</span>
<span class="sd">      grad_fun: Optional gradient oracle corresponding to the provided objective function ``fun``. The gradient function returns the gradient of the objective function with respect to its first argument (*i.e.* the optimization variable). The gradient function should have the same function signature as the objective function, and the returned gradient should have the same shapes and types as the the optimization variable (*i.e.* the first argument of ``fun``).</span>
<span class="sd">      hvp_fun: Optional Hessian-vector product oracle. The hvp function should take optimization variable and vector (of the same shape and type) as its first and second arguments, respectively. It is also expected to have the same additional arguments as the provided objective function ``fun``. The returned result should have the same shapes and types as the the optimization variable (*i.e.* the first argument of ``fun``).</span>
<span class="sd">      pre_update: Optional function to execute before optimizer&#39;s each update on the iterate. The function expects signature ``params, state = pre_update(params, state, *args, **kwargs)`` where ``state`` is the :class:`SketchyKatyushaState` object.</span>
<span class="sd">      rho: Regularization parameter. Expect a non-negative value.</span>
<span class="sd">      rank: Rank of the preconditioner. Expect a positive value.</span>
<span class="sd">      mu: Strong convexity parameter. Expect a positive value.</span>
<span class="sd">      grad_batch_size: Size of the batch of data to compute stochastic gradient at each iteration. Expect a positive value.</span>
<span class="sd">      hess_batch_size: Size of the batch of data to estimate the stochastic Hessian when updating the preconditioner. Expect a positive value.</span>
<span class="sd">      update_freq: Update frequency of the preconditioner. When set to ``0`` or :math:`\infty` (*e.g.* ``jax.numpy.inf`` or</span>
<span class="sd">        ``numpy.inf``), the optimizer uses constant preconditioner that is constructed at</span>
<span class="sd">        the beginning of the optimization process.</span>
<span class="sd">      snapshop_update_prob: Probability of updating the snapshot. Expect a value in :math:`(0,1)`.</span>
<span class="sd">      seed: Initial seed for the random number generator.</span>
<span class="sd">      momentum_param: Momentum parameter (default ``1/2``).</span>
<span class="sd">      momentum_multiplier: Momentum multiplier (default ``2/3``).</span>
<span class="sd">      maxiter: Maximum number of iterations to run the optimizer (default ``20``). Expect a positive value.</span>
<span class="sd">      tol: Threshold of the gradient norm used for terminating the optimizer (default ``1e-3``).</span>
<span class="sd">      verbose: Whether to print diagnostic message (default ``false``).</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">mu</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">momentum_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">momentum_multiplier</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">3</span>
    <span class="n">snapshop_update_prob</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="nf">_init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SketchyKatyushaState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function initializes the optimizer state.&quot;&quot;&quot;</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">full_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
        <span class="n">full_grad</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ravel_tree</span><span class="p">(</span><span class="n">full_grad</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">SketchyKatyushaState</span><span class="p">(</span>
            <span class="n">iter_num</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">value</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">),</span>
            <span class="n">error</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">),</span>
            <span class="n">key</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">),</span>
            <span class="n">precond</span><span class="o">=</span><span class="k">lambda</span> <span class="n">g</span><span class="p">:</span> <span class="n">g</span><span class="p">,</span>
            <span class="n">full_grad</span><span class="o">=</span><span class="n">full_grad</span><span class="p">,</span>
            <span class="n">snapshot</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
            <span class="n">z</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
            <span class="n">step_size</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">L</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">theta</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_step_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">labda</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function overrides the superclass&#39;s method and updates relevant values using the provided preconditioned smoothness constant labda.&quot;&quot;&quot;</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">/</span> <span class="n">labda</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
            <span class="mf">0.5</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum_multiplier</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">step_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum_param</span> <span class="o">/</span> <span class="p">(</span><span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum_param</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">state</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span>
            <span class="n">step_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span>
            <span class="n">L</span><span class="o">=</span><span class="n">labda</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_snapshot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function updates snapshot and full gradient with specified update probability.&quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">update</span><span class="p">():</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">full_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
            <span class="n">full_grad</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ravel_tree</span><span class="p">(</span><span class="n">full_grad</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">state</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">full_grad</span><span class="o">=</span><span class="n">full_grad</span><span class="p">,</span> <span class="n">snapshot</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">u</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">snapshop_update_prob</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SolverState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function performs an update on the iterate.&quot;&quot;&quot;</span>
        <span class="c1"># generate a random batch</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">subkey1</span><span class="p">,</span> <span class="n">subkey2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="n">subkey1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_batch_size</span><span class="p">,),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey2</span><span class="p">)</span>

        <span class="c1"># call custom pre-update function</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_update</span><span class="p">:</span>
            <span class="n">params</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_update</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="c1"># compute negative momentum</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tree_scalar_mul</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">z</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tree_add_scalar_mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum_param</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">snapshot</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tree_add_scalar_mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">state</span><span class="o">.</span><span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum_param</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

        <span class="c1"># compute preconditioned variance-reduced accelerated stochastic gradient</span>
        <span class="n">value</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">grad_snapshot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad</span><span class="p">(</span>
            <span class="n">state</span><span class="o">.</span><span class="n">snapshot</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">error</span> <span class="o">=</span> <span class="n">tree_l2_norm</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">unraveled_grad</span><span class="p">,</span> <span class="n">unravel_fun</span> <span class="o">=</span> <span class="n">ravel_tree</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
        <span class="n">unraveled_grad_snapshot</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ravel_tree</span><span class="p">(</span><span class="n">grad_snapshot</span><span class="p">)</span>

        <span class="n">direction</span> <span class="o">=</span> <span class="n">unravel_fun</span><span class="p">(</span>
            <span class="n">state</span><span class="o">.</span><span class="n">precond</span><span class="p">(</span><span class="n">unraveled_grad</span> <span class="o">-</span> <span class="n">unraveled_grad_snapshot</span> <span class="o">+</span> <span class="n">state</span><span class="o">.</span><span class="n">full_grad</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># update snapshot</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_snapshot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># perform an update</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">tree_scalar_mul</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">state</span><span class="o">.</span><span class="n">sigma</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">tree_add</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">z</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">tree_add_scalar_mul</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="o">-</span><span class="n">state</span><span class="o">.</span><span class="n">step_size</span> <span class="o">/</span> <span class="n">state</span><span class="o">.</span><span class="n">L</span><span class="p">,</span> <span class="n">direction</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">tree_scalar_mul</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">state</span><span class="o">.</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">state</span><span class="o">.</span><span class="n">sigma</span><span class="p">),</span> <span class="n">z</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">tree_add_scalar_mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">tree_sub</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">z</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">SolverState</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
            <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span>
                <span class="n">iter_num</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="n">error</span><span class="p">,</span>
                <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
                <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

<div class="viewcode-block" id="SketchyKatyusha.run">
<a class="viewcode-back" href="../../api/sketchyopts.solver.html#sketchyopts.solver.SketchyKatyusha.run">[docs]</a>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init_params</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SolverState</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The function runs the optimization loop.</span>

<span class="sd">        Args:</span>
<span class="sd">          init_params: Initial value of the optimization variable.</span>
<span class="sd">          data: Full dataset. Expect an array of shape ``(num_samples, ...)``.</span>
<span class="sd">          *args: Additional positional arguments to be passed to ``fun`` (and ``grad_fun`` as well as ``hvp_fun`` if provided).</span>
<span class="sd">          **kwargs: Additional keyword arguments to be passed to ``fun`` (and ``grad_fun`` as well as ``hvp_fun`` if provided).</span>
<span class="sd">        Returns:</span>
<span class="sd">          Final optimization variable and solver state. The variable has the same shape as the provided initial value ``init_params``, and the state is an :class:`SketchyKatyushaState` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># initialize iterate and state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">init_params</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_state</span><span class="p">(</span><span class="n">init_params</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># jit the parameter update function</span>
        <span class="n">update_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">p</span><span class="p">,</span> <span class="n">s</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_params</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># run the optimization loop</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">maxiter</span><span class="p">):</span>

            <span class="c1"># update preconditioner</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_freq</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_precond</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="c1"># update iterate</span>
            <span class="n">params</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">update_params</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

            <span class="c1"># break out of loop if tolerance has been reached</span>
            <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">error</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                <span class="n">jax</span><span class="o">.</span><span class="n">debug</span><span class="o">.</span><span class="n">print</span><span class="p">(</span>
                    <span class="s2">&quot;Info: early termination because error tolerance has been reached.&quot;</span>
                <span class="p">)</span>
                <span class="k">break</span>

        <span class="k">return</span> <span class="n">SolverState</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span></div>
</div>

</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>